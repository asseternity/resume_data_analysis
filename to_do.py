# PROJECT 1 - cleaning and visualizing
# v find data that interests me
# v set up a Python environment with Pandas, Matplotlib, Seaborn, NumPy, and Jupyter Notebooks
# v figure out how to utilize SQL / join tables in Python environments
# v clean the data (handle missing values, ensure correct data types)
# v do the above for all tables available in the dataset
# v use sql to find cool shit with SOME NUMBERS
# v use Pandas for statistical analysis: data.describe(), data.corr()
# v create visualizations of relationships with Matplotlib/Seaborn (scatter plots, bar charts, histograms).
# v do more SQL queries and visualizations - a useful analysis of CAREER TRANSITIONS! How many had law degrees, if any?
# understand how to do visualizations
# remember and do some correlated and non-correlated subqueries
# do more visualizations: (1) breakdown of positions, like which degrees do data scientists have? (2) which roles do law degrees have?
# find and incorporate more job or resume data from kagglehub
# compile everything into a Jupyter Notebook
# deploy

# PROJECT 2 - machine learning
# select a machine learning problem
# set up your environment: Install Scikit-learn, Pandas, Matplotlib, Jupyter Notebooks.
# perform basic data cleaning (handle missing values, drop duplicates).
# feature engineering (adapt the machine learning library - which?)
# split data into train/test sets using train_test_split
# choose model based on problem type: Classification: Logistic Regression/Random Forest. Regression: Linear Regression/Random Forest Regressor
# evaluate model performance: Classification: Accuracy/Precision/Recall/Confusion Matrix. Regression: Mean Squared Error/R-squared
# perform Hyperparameter Tuning using GridSearchCV or RandomizedSearchCV to improve the model
# document everything in a Jupyter Notebook