# PROJECT 1 - cleaning and visualizing
# v find data that interests me
# v set up a Python environment with Pandas, Matplotlib, Seaborn, NumPy, and Jupyter Notebooks
# v figure out how to utilize SQL / join tables in Python environments
# v clean the data (handle missing values, ensure correct data types)
# v do the above for all tables available in the dataset
# v use sql to find cool shit with SOME NUMBERS
# v use Pandas for statistical analysis: data.describe(), data.corr()
# v create visualizations of relationships with Matplotlib/Seaborn (scatter plots, bar charts, histograms).
# v do more SQL queries and visualizations - a useful analysis of CAREER TRANSITIONS! How many had law degrees, if any?
# v come up with: (1) what more visualizations can I do for this project and (2) how will correlated and non-correlated subqueries help here 
# v Q: which field of IT do lawyers go into?
# v Q: the opposite - what degrees do lawyers have?
# v make SQL queries about the above
# understand how to do visualizations and visualize the above
# enhance visualizations with seaborn
# compile everything into a Jupyter Notebook, e.g.:

# üìä What Do Lawyers Do in Tech?

## 1. Introduction
# - Motivation
# - Dataset source

## 2. Data Loading and Setup

## 3. Data Cleaning (with reusable functions)

## 4. Exploratory Data Analysis (EDA)
# - üìà Degree distributions
# - üîç What degrees do lawyers have?
# - üíº What jobs do lawyers get in tech?

## 5. Visualizations
# - Beautiful bar charts with seaborn/matplotlib
# - Maybe 1 interactive chart (plotly)

## 6. Insights and Conclusions
# - Summary of findings
# - Surprising results

## 7. Next Steps (for future work)

# *--- HERE IS WHAT DATA SCIENCE IS: ---*
# 1) find some data in csv / on kaaggle / from an API / wherever
# 2) clean the data from duplicates, from Nulls, make all lowercase, combine all wrong data into one category, etc.
# 3) come up with an interesting question for the data
# 4) write an elaborate SQL query to get a table with that information
# 5) run the sql query with pandasql.sqldf
# 6) use pandas to group the data using value_counts(), isin(), etc. 
# 7) visualize that information with plt

# PROJECT 2 - machine learning
# select a machine learning problem
# set up your environment: Install Scikit-learn, Pandas, Matplotlib, Jupyter Notebooks.
# perform basic data cleaning (handle missing values, drop duplicates).
# feature engineering (adapt the machine learning library - which?)
# split data into train/test sets using train_test_split
# choose model based on problem type: Classification: Logistic Regression/Random Forest. Regression: Linear Regression/Random Forest Regressor
# evaluate model performance: Classification: Accuracy/Precision/Recall/Confusion Matrix. Regression: Mean Squared Error/R-squared
# perform Hyperparameter Tuning using GridSearchCV or RandomizedSearchCV to improve the model
# document everything in a Jupyter Notebook